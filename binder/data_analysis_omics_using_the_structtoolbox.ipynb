{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "centered-parks",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The 'structToolbox' includes an extensive set of data (pre-)processing and analysis tools for metabolomics and other omics, with a strong emphasis on statistics and machine learning. The methods and tools have been implemented using class-based templates available via the `struct` (Statistics in R Using Class-based Templates) package. The aim of this vignette is to introduce the reader to basic and more advanced structToolbox-based operations and implementations, such as the use of `struct` objects, getting/setting methods/parameters, and building workflows for the analysis of mass spectrometry (MS) and nuclear magnetic resonance (NMR)-based Metabolomics and proteomics datasets. The workflows demonstrated here include a wide range of methods and tools including pre-processing such as filtering, normalisation and scaling, followed by univariate and/or multivariate statistics, and machine learning approaches. \n",
    "\n",
    "# Getting started\n",
    "The latest version of `structToolbox` compatible with your current R version can be installed using `BiocManager`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-talent",
   "metadata": {
    "eval": false,
    "include": true
   },
   "outputs": [],
   "source": [
    "# install BiocManager if not present\n",
    "# if (!requireNamespace(\"BiocManager\", quietly = TRUE))\n",
    "#     install.packages(\"BiocManager\")\n",
    "\n",
    "# install structToolbox and dependencies\n",
    "# BiocManager::install(\"structToolbox\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-volleyball",
   "metadata": {},
   "source": [
    "A number of additional packages are needed for this vignette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-wrist",
   "metadata": {
    "message": false,
    "warning": false
   },
   "outputs": [],
   "source": [
    "## install additional bioc packages for vignette if needed\n",
    "#BiocManager::install(c('pmp', 'ropls', 'BiocFileCache'))\n",
    "\n",
    "## install additional CRAN packages if needed\n",
    "#install.packages(c('cowplot', 'openxlsx'))\n",
    "\n",
    "suppressPackageStartupMessages({\n",
    "    # Bioconductor packages\n",
    "    library(structToolbox)\n",
    "    library(pmp)\n",
    "    library(ropls)\n",
    "    library(BiocFileCache)\n",
    "  \n",
    "    # CRAN libraries\n",
    "    library(ggplot2)\n",
    "    library(gridExtra)\n",
    "    library(cowplot)\n",
    "    library(openxlsx)\n",
    "})\n",
    "\n",
    "# use the BiocFileCache\n",
    "bfc <- BiocFileCache(ask = FALSE)",
    "\n",
    "set.seed(57475)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-cedar",
   "metadata": {},
   "source": [
    "# Introduction to `struct` objects, including models, model sequences, model charts and ontology.\n",
    "\n",
    "## Introduction\n",
    "PCA (Principal Component Analysis) and PLS (Partial Least Squares) are commonly applied methods for exploring and analysing multivariate datasets. Here we use these two statistical methods to demonstrate the different types of `struct` (STatistics in R Using Class Templates) objects that are available as part of the `structToolbox` and how these objects (i.e. class templates) can be used to conduct unsupervised and supervised multivariate statistical analysis.\n",
    "\n",
    "## Dataset\n",
    "For demonstration purposes we will use the \"Iris\" dataset. This famous (Fisher's or Anderson's) dataset contains measurements of sepal length and width and petal length and width, in centimeters, for 50 flowers from each of 3 class of Iris. The class are Iris setosa, versicolor, and virginica. See here (https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/iris.html) for more information.\n",
    "\n",
    "Note: this vignette is also compatible with the Direct infusion mass spectrometry metabolomics \"benchmark\" dataset described in Kirwan et al., _Sci Data_ *1*, 140012 (2014) (https://doi.org/10.1038/sdata.2014.12).\n",
    "\n",
    "Both datasets are available as part of the structToolbox package and already prepared as a `DatasetExperiment` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Iris dataset (comment if using MTBLS79 benchmark data)\n",
    "D = iris_DatasetExperiment()\n",
    "D$sample_meta$class = D$sample_meta$Species\n",
    "\n",
    "## MTBLS (comment if using Iris data)\n",
    "# D = MTBLS79_DatasetExperiment(filtered=TRUE)\n",
    "# M = pqn_norm(qc_label='QC',factor_name='sample_type') + \n",
    "#   knn_impute(neighbours=5) +\n",
    "#   glog_transform(qc_label='QC',factor_name='sample_type') +\n",
    "#   filter_smeta(mode='exclude',levels='QC',factor_name='sample_type')\n",
    "# M = model_apply(M,D)\n",
    "# D = predicted(M)\n",
    "\n",
    "# show info\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-scheduling",
   "metadata": {},
   "source": [
    "### DatasetExperiment objects\n",
    "The `DatasetExperiment` object is an extension of the `SummarizedExperiment` class used by the Bioconductor community. It contains three main parts:\n",
    "\n",
    "1. `data` A data frame containing the measured data for each sample.\n",
    "2. `sample_meta` A data frame of additional information related to the samples e.g. group labels.\n",
    "3. `variable_meta` A data frame of additional information related to the variables (features) e.g. annotations\n",
    "\n",
    "Like all `struct` objects it also contains `name` and `description` fields (called \"slots\" is R language). \n",
    "\n",
    "A key difference between `DatasetExperiment` and `SummarizedExperiment` objects is that the data is transposed. i.e. for `DatasetExperiment` objects the samples are in rows and the features are in columns, while the opposite is true for `SummarizedExperiment` objects.\n",
    "\n",
    "All slots are accessible using dollar notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show some data\n",
    "head(D$data[,1:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-algorithm",
   "metadata": {},
   "source": [
    "## Using `struct` model objects\n",
    "\n",
    "### Statistical models\n",
    "Before we can apply e.g. PCA we first need to create a PCA object. This object contains all the inputs, outputs and methods needed to apply PCA. We can set parameters such as the number of components when the PCA model is created, but we can also use dollar notation to change/view it later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = PCA(number_components=15)\n",
    "P$number_components=5\n",
    "P$number_components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-measurement",
   "metadata": {},
   "source": [
    "The inputs for a model can be listed using `param_ids(object)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-remainder",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "param_ids(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-bryan",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Or a summary of the object can be printed to the console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-usage",
   "metadata": {},
   "outputs": [],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-glenn",
   "metadata": {},
   "source": [
    "### Model sequences\n",
    "Unless you have good reason not to, it is usually sensible to mean centre the columns of the data before PCA. Using the `STRUCT` framework we can create a model sequence that will mean centre and then apply PCA to the mean centred data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = mean_centre() + PCA(number_components = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-george",
   "metadata": {},
   "source": [
    "In `structToolbox` mean centring and PCA are both model objects, and joining them using \"+\" creates a model_sequence object. In a model_sequence the outputs of the first object (mean centring) are automatically passed to the inputs of the second object (PCA), which allows you chain together modelling steps in order to build a workflow.\n",
    "\n",
    "The objects in the model_sequence can be accessed by indexing, and we can combine this with dollar notation. For example, the PCA object is the second object in our sequence and we can access the number of components as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "M[2]$number_components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-exercise",
   "metadata": {},
   "source": [
    "### Training/testing models\n",
    "Model and model_sequence objects need to be trained using data in the form of a `DatasetExperiment` object. For example, the PCA model sequence we created (`M`) can be trained using the iris `DatasetExperiment` object ('D'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-association",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "M = model_train(M,D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-chemistry",
   "metadata": {},
   "source": [
    "This model sequence has now mean centred the original data and calculated the PCA scores and loadings.\n",
    "  \n",
    "Model objects can be used to generate predictions for test datasets. For the PCA model sequence this involves mean centring the test data using the mean of training data, and the projecting the centred test data onto the PCA model using the loadings. The outputs are all stored in the model sequence and can be accessed using dollar notation. For this example we will just use the training data again (sometimes called autoprediction), which for PCA allows us to explore the training data in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-moment",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "M = model_predict(M,D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-editor",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "rising-letter",
   "metadata": {},
   "source": [
    "Sometimes models don't make use the training/test approach e.g. univariate statsitics, filtering etc. For these models the `model_apply` method can be used instead. For models that do provide training/test methods, `model_apply` applies autoprediction by default i.e. it is a short-cut for applying `model_train` and `model_predict` to the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-miller",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = model_apply(M,D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-florence",
   "metadata": {},
   "source": [
    "The available outputs for an object can be listed and accessed like input params, using dollar notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-filing",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_ids(M[2])\n",
    "M[2]$scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-botswana",
   "metadata": {},
   "source": [
    "### Model charts\n",
    "The `struct` framework includes chart objects. Charts associated with a model object can be listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_names(M[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-bouquet",
   "metadata": {},
   "source": [
    "Like model objects, chart objects need to be created before they can be used. Here we will plot the PCA scores plot for our mean centred PCA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = pca_scores_plot(factor_name='class') # colour by class\n",
    "chart_plot(C,M[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-president",
   "metadata": {},
   "source": [
    "Note that indexing the PCA model is required because the `pca_scores_plot` object requires a PCA object as input, not a model_sequence.\n",
    "\n",
    "If we make changes to the input parameters of the chart, `chart_plot` must be called again to see the effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-cycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add petal width to meta data of pca scores\n",
    "M[2]$scores$sample_meta$example=D$data[,1]\n",
    "# update plot\n",
    "C$factor_name='example'\n",
    "chart_plot(C,M[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "owned-reader",
   "metadata": {},
   "source": [
    "The `chart_plot` method returns a ggplot object so that you can easily combine it with other plots using the `gridExtra` or `cowplot` packages for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-saver",
   "metadata": {
    "fig.width": 10
   },
   "outputs": [],
   "source": [
    "# scores plot\n",
    "C1 = pca_scores_plot(factor_name='class') # colour by class\n",
    "g1 = chart_plot(C1,M[2])\n",
    "\n",
    "# scree plot\n",
    "C2 = pca_scree_plot()\n",
    "g2 = chart_plot(C2,M[2])\n",
    "\n",
    "# arange in grid\n",
    "grid.arrange(grobs=list(g1,g2),nrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-harvard",
   "metadata": {},
   "source": [
    "### STATistics Ontology (STATO)\n",
    "Within the `struct` framework (and `structToolbox`) we use STATO objects to provide standardardised definitions for objects and its inputs and outputs.\n",
    "\n",
    "STATO is a general purpose STATistics Ontology (http://stato-ontology.org). From the webpage:\n",
    "\n",
    "> Its aim is to provide coverage for processes such as statistical tests, their conditions of application, and information needed or resulting from statistical methods, such as probability distributions, variables, spread and variation metrics. STATO also covers aspects of experimental design and description of plots and graphical representations commonly used to provide visual cues of data distribution or layout and to assist review of the results.\n",
    "\n",
    "The STATistics Ontology for the PCA model is available via the following methods. We can verify that an object has STATO definitions attached by checking that the object is derived from the `stat` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-value",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "is(PCA(),'stato')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-execution",
   "metadata": {},
   "source": [
    "STATO definitions can be extracted from the object using the `stato_id`, `stato_name` and `stato_definition` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-entry",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# this is the stato id for PCA\n",
    "stato_id(M[2])\n",
    "\n",
    "# this is the stato name\n",
    "stato_name(M[2])\n",
    "\n",
    "# this is the stato definition\n",
    "stato_definition(M[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-muscle",
   "metadata": {},
   "source": [
    "This information is more succinctly displayed using `stato_summary`. This method also scans over all input params and outputs for those with STATO definitions and displays those as well. \n",
    "\n",
    "For PCA the \"number of components\" input is listed with definitions because it is a STATO object, but none of the outputs (e.g. PCA scores) are STATO objects at this time and therefore no definition is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "stato_summary(M[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-broadcasting",
   "metadata": {},
   "source": [
    "## Validating supervised statistical models\n",
    "Validation is an important aspect of chemometric modelling. The `struct` framework enables this kind of iterative model testing through `iterator` objects. \n",
    "\n",
    "### Cross-validation\n",
    "Cross validation is a common technique for assessing the performance of classification models. For this example we will use a Partial least squares-discriminant analysis (PLS-DA) model. Data should be mean centred prior to PLS, so we will build a model sequence first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = mean_centre() + PLSDA(number_components=2,factor_name='class')\n",
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-artist",
   "metadata": {},
   "source": [
    "`iterator` objects like the k-fold cross-validation object (`kfold_xval`) can be created just like any other `struct` object. Parameters can be set at creation using the equals sign, and accessed or changed later using dollar notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object\n",
    "XCV = kfold_xval(folds=5,factor_name='class')\n",
    "\n",
    "# change the number of folds\n",
    "XCV$folds=10\n",
    "XCV$folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-favor",
   "metadata": {},
   "source": [
    "The model to be cross-validated can be set/accessed using the `models` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "models(XCV)=M\n",
    "models(XCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-equation",
   "metadata": {},
   "source": [
    "Alternatively, iterators can be combined with models using the multiplication symbol was shorthand for the `models` assignement method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-dictionary",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# cross validation of a mean centred PLSDA model\n",
    "XCV = kfold_xval(\n",
    "        folds=5,\n",
    "        method='venetian',\n",
    "        factor_name='class') * \n",
    "      (mean_centre() + PLSDA(factor_name='class'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sacred-timothy",
   "metadata": {},
   "source": [
    "The `run` method can be used with any `iterator` object. The iterator will then run the set model or model sequence multiple times. \n",
    "\n",
    "In this case we run cross-validation 5 times, splitting the data into different training and test sets each time. \n",
    "\n",
    "The `run` method also needs a `metric` to be specified, which is another type of `struct` object. This metric may be calculated once after all iterations, or after each iteration, depending on the iterator type (resampling, permutation etc). For cross-validation we will calculate \"balanced accuracy\" after all iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-speech",
   "metadata": {},
   "outputs": [],
   "source": [
    "XCV = run(XCV,D,balanced_accuracy())\n",
    "XCV$metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-magic",
   "metadata": {},
   "source": [
    "Note The `balanced_accuracy` metric actually reports 1-accuracy, so a value of 0 indicates perfect performance. The standard deviation \"sd\" is NA in this example because there is only one permutation.\n",
    "\n",
    "Like other `struct` objects, iterators can have chart objects associated with them. The `chart_names` function will list them for an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_names(XCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-department",
   "metadata": {},
   "source": [
    "Charts for `iterator` objects can be plotted in the same way as charts for any other object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-roller",
   "metadata": {
    "fig.height": 4,
    "fig.width": 8,
    "warning": false
   },
   "outputs": [],
   "source": [
    "C = kfoldxcv_grid(\n",
    "  factor_name='class',\n",
    "  level=levels(D$sample_meta$class)[2]) # first level\n",
    "chart_plot(C,XCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-harmony",
   "metadata": {},
   "source": [
    "It is possible to combine multiple iterators by using the multiplication symbol. This is equivalent to nesting one iterator inside the other. For example, we can repeat our cross-validation multiple times by permuting the sample order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-sense",
   "metadata": {},
   "outputs": [],
   "source": [
    "# permute sample order 10 times and run cross-validation\n",
    "P = permute_sample_order(number_of_permutations = 10) * \n",
    "    kfold_xval(folds=5,factor_name='class')*\n",
    "    (mean_centre() + PLSDA(factor_name='class',number_components=2))\n",
    "P = run(P,D,balanced_accuracy())\n",
    "P$metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-restaurant",
   "metadata": {},
   "source": [
    "# A typical workflow for processing and analysing mass spectrometry-based metabolomics data.\n",
    "\n",
    "## Introduction\n",
    "This vignette provides an overview of a `structToolbox` workflow implemented to process (e.g. filter features, signal drift and batch correction, normalise and missing value imputation) mass spectrometry data. The workflow exists of methods that are part of the Peak Matrix Processing (`r Biocpkg('pmp')`) package, including a range of additional filters that are described in Kirwan et al., [2013](https://link.springer.com/article/10.1007/s00216-013-6856-7), [2014](https://doi.org/10.1038/sdata.2014.12).\n",
    "\n",
    "Some packages are required for this vignette in addition `structToolbox`:\n",
    "\n",
    "## Dataset\n",
    "For demonstration purposes we will process and analyse the MTBLS79 dataset ('Dataset 7:SFPM' Kirwan et al., [2014](https://doi.org/10.1038/sdata.2014.12). This dataset represents a systematic evaluation of the reproducibility of a multi-batch direct-infusion mass spectrometry (DIMS)-based metabolomics study of cardiac tissue extracts. It comprises twenty biological samples (cow vs. sheep) that were analysed repeatedly, in 8 batches across 7 days, together with a concurrent set of quality control (QC) samples. Data are presented from each step of the data processing workflow and are available through MetaboLights (https://www.ebi.ac.uk/metabolights/MTBLS79). \n",
    "\n",
    "The `MTBLS79_DatasetExperiment` object included in the `structToolbox` package is a processed version of the MTBLS79 dataset available in peak matrix processing (`r Biocpkg('pmp')`) package. This vignette describes step by step how the `structToolbox` version was created from the `pmp` version (i.e. 'Dataset 7:SFPM' from the Scientific Data publication - https://doi.org/10.1038/sdata.2014.12).\n",
    "\n",
    "The `SummarizedExperiment` object from the `pmp` package needs to be converted to a `DatasetExperiment` object for use with `structToolbox`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-antarctica",
   "metadata": {
    "message": false,
    "warning": false
   },
   "outputs": [],
   "source": [
    "# the pmp SE object\n",
    "SE = MTBLS79\n",
    "\n",
    "# convert to DE\n",
    "DE = as.DatasetExperiment(SE)\n",
    "DE$name = 'MTBLS79'\n",
    "DE$description = 'Converted from SE provided by the pmp package'\n",
    "\n",
    "# add a column indicating the order the samples were measured in\n",
    "DE$sample_meta$run_order = 1:nrow(DE)\n",
    "\n",
    "# add a column indicating if the sample is biological or a QC\n",
    "Type=as.character(DE$sample_meta$Class)\n",
    "Type[Type != 'QC'] = 'Sample'\n",
    "DE$sample_meta$Type = factor(Type)\n",
    "\n",
    "# convert to factors\n",
    "DE$sample_meta$Batch = factor(DE$sample_meta$Batch)\n",
    "DE$sample_meta$Class = factor(DE$sample_meta$Class)\n",
    "\n",
    "# print summary\n",
    "DE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-activation",
   "metadata": {},
   "source": [
    "Full processing of the data set requires a number of steps. These will be applied using a single `struct` model sequence (`model.seq`).\n",
    "\n",
    "## Signal drift and batch correction\n",
    "A batch correction algorithm is applied to reduce intra- and inter- batch variations in the dataset.\n",
    "Quality Control-Robust Spline Correction (QC-RSC) is provided in the `pmp` package, and it has been\n",
    "wrapped into a `structToolbox` object called `sb_corr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-helicopter",
   "metadata": {
    "message": false,
    "warning": false
   },
   "outputs": [],
   "source": [
    "\n",
    "M = # batch correction\n",
    "    sb_corr(\n",
    "      order_col='run_order',\n",
    "      batch_col='Batch', \n",
    "      qc_col='Type', \n",
    "      qc_label='QC'\n",
    "    )\n",
    "\n",
    "M = model_apply(M,DE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-acrylic",
   "metadata": {},
   "source": [
    "The figure below shows a plot of a feature vs run order, before and after the correction. It can be seen that the correction has removed instrument drift within and between batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-return",
   "metadata": {
    "fig.wide": true,
    "fig.width": 10,
    "warning": false
   },
   "outputs": [],
   "source": [
    "C = feature_profile(\n",
    "      run_order='run_order',\n",
    "      qc_label='QC',\n",
    "      qc_column='Type',\n",
    "      colour_by='Batch',\n",
    "      feature_to_plot='200.03196'\n",
    "  )\n",
    "\n",
    "# plot and modify using ggplot2 \n",
    "chart_plot(C,DE)+ylab('Peak area')+ggtitle('Before')\n",
    "chart_plot(C,predicted(M))+ylab('Peak area')+ggtitle('After')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-onion",
   "metadata": {},
   "source": [
    "An additional step is added to the published workflow to remove any feature not corrected by QCRCMS. This can occur if there are not enough measured QC values within a batch. `QCRMS` in the `pmp` package currently returns NA for all samples in the feature where this occurs. Features where this occurs will be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-balance",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "M2 = filter_na_count(\n",
    "      threshold=3,\n",
    "      factor_name='Batch'\n",
    "    )\n",
    "M2 = model_apply(M2,predicted(M))\n",
    "\n",
    "# calculate number of features removed\n",
    "nc = ncol(DE) - ncol(predicted(M2))\n",
    "\n",
    "cat(paste0('Number of features removed: ', nc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-springer",
   "metadata": {},
   "source": [
    "The output of this step is the output of `MTBLS79_DatasetExperiment(filtered=FALSE)`.\n",
    "\n",
    "## Feature filtering\n",
    "\n",
    "In the journal article three spectral cleaning steps are applied. In the first filter a Kruskal-Wallis test is used to identify features not reliably detected in the QC samples (p < 0.0001) of all batches. We follow the same parameters as the original article and do not use multiple test correction (`mtc = 'false'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-serial",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "M3 = kw_rank_sum(\n",
    "      alpha=0.0001,\n",
    "      mtc='none',\n",
    "      factor_names='Batch',\n",
    "      predicted='significant'\n",
    "    ) +\n",
    "    filter_by_name(\n",
    "      mode='exclude',\n",
    "      dimension = 'variable',\n",
    "      seq_in = 'names', \n",
    "      names='seq_fcn', # this is a placeholder and will be replaced by seq_fcn\n",
    "      seq_fcn=function(x){return(x[,1])}\n",
    "    )\n",
    "M3 = model_apply(M3, predicted(M2))\n",
    "\n",
    "nc = ncol(predicted(M2)) - ncol(predicted(M3))\n",
    "cat(paste0('Number of features removed: ', nc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-december",
   "metadata": {},
   "source": [
    "To make use of univariate tests such as `kw_rank_sum` as a filter some advanced features of `struct` are needed. Slots `predicted`, and `seq_in` are used to ensure the correct output of the univariate test is connected to the correct input of a feature filter using `filter_by_name`. Another slot `seq_fcn` is used to extract the relevant column of the `predicted` output so that it is compatible with the `seq_in` input. A placeholder is used for the \"names\" parameter (`names = 'place_holder'`) as this input will be replaced by the output from `seq_fcn`.\n",
    "\n",
    "The second filter is a Wilcoxon Signed-Rank test. It is used to identify features that are not representative of the average of the biological samples (p < 1e-14). Again we make use of `seq_in` and `seq_fcn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-canvas",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "M4 = wilcox_test(\n",
    "      alpha=1e-14,\n",
    "      factor_names='Type', \n",
    "      mtc='none', \n",
    "      predicted = 'significant'\n",
    "    ) +\n",
    "    filter_by_name(\n",
    "      mode='exclude',\n",
    "      dimension='variable',\n",
    "      seq_in='names', \n",
    "      names='place_holder'\n",
    "    )\n",
    "M4 = model_apply(M4, predicted(M3))\n",
    "\n",
    "nc = ncol(predicted(M3)) - ncol(predicted(M4))\n",
    "cat(paste0('Number of features removed: ', nc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-andorra",
   "metadata": {},
   "source": [
    "Finally, an RSD filter is used to remove features with high analytical variation (QC RSD > 20 removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "M5 = rsd_filter(\n",
    "     rsd_threshold=20,\n",
    "     factor_name='Type'\n",
    ")\n",
    "M5 = model_apply(M5,predicted(M4))\n",
    "\n",
    "nc = ncol(predicted(M4)) - ncol(predicted(M5))\n",
    "cat(paste0('Number of features removed: ', nc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-potential",
   "metadata": {},
   "source": [
    "The output of this filter is the output of `MTBLS79_DatasetExperiment(filtered=TRUE)`.\n",
    "\n",
    "## Normalisation, missing value imputation and scaling\n",
    "We will apply a number of common pre-processing steps to the filtered peak matrix that are identical to steps applied in are described in Kirwan et al. [2013](https://link.springer.com/article/10.1007/s00216-013-6856-7), [2014](https://doi.org/10.1038/sdata.2014.12).\n",
    "\n",
    "- Probabilistic Quotient Normalisation (PQN)\n",
    "- k-nearest neighbours imputation (k = 5)\n",
    "- Generalised log transform (glog)\n",
    "\n",
    "These steps prepare the data for multivariate analysis by accounting for sample concentration differences, imputing missing values and scaling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak matrix processing\n",
    "M6 = pqn_norm(qc_label='QC',factor_name='Type') + \n",
    "     knn_impute(neighbours=5) +\n",
    "     glog_transform(qc_label='QC',factor_name='Type')\n",
    "M6 = model_apply(M6,predicted(M5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-highland",
   "metadata": {},
   "source": [
    "## Exploratory Analysis\n",
    "Principal Component Analysis (PCA) can be used to visualise high-dimensional data. It is an unsupervised method that maximises variance in a reduced number of latent variables, or principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "M7  = mean_centre() + PCA(number_components = 2)\n",
    "\n",
    "# apply model sequence to data\n",
    "M7 = model_apply(M7,predicted(M6))\n",
    "\n",
    "# plot pca scores\n",
    "C = pca_scores_plot(factor_name=c('Sample_Rep','Class'),ellipse='none')\n",
    "chart_plot(C,M7[2]) + coord_fixed() +guides(colour=FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-perfume",
   "metadata": {},
   "source": [
    "This plot is very similar to Figure 3b of the original publication [link](https://www.nature.com/articles/sdata201412/figures/3). Sample replicates are represented by colours and samples groups  (C = cow and S = Sheep) by different shapes.\n",
    "\n",
    "Plotting the scores and colouring by Batch indicates that the signal/batch correction was effective as all batches are overlapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart object\n",
    "C = pca_scores_plot(factor_name=c('Batch'),ellipse='none')\n",
    "# plot\n",
    "chart_plot(C,M7[2]) + coord_fixed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-presence",
   "metadata": {},
   "source": [
    "# Partial Least Squares (PLS) analysis of a untargeted LC-MS-based clinical metabolomics dataset.\n",
    "\n",
    "## Introduction\n",
    "The aim of this vignette is to demonstrate how to 1) apply and validate Partial Least Squares (PLS) analysis using the structToolbox, 2) reproduce statistical analysis in [Thevenot et al. (2015)](https://pubs.acs.org/doi/10.1021/acs.jproteome.5b00354) and 3. compare different implementations of PLS.\n",
    "\n",
    "## Dataset\n",
    "The objective of the original study was to:\n",
    ">  ...study the influence of age, body mass index (bmi), and gender on metabolite concentrations in urine, by analysing 183 samples from a cohort of adults with liquid chromatography coupled to high-resolution mass spectrometry.\n",
    "\n",
    "[Thevenot et al. (2015)](https://pubs.acs.org/doi/10.1021/acs.jproteome.5b00354)\n",
    "\n",
    "The \"Sacurine\" dataset needs to be converted to a `DatasetExperiment` object. The `r Biocpkg(\"ropls\")` package provides the data as a list containing a `dataMatrix`, `sampleMetadata` and `variableMetadata`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-violation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data('sacurine',package = 'ropls')\n",
    "# the 'sacurine' list should now be available\n",
    "\n",
    "# move the annotations to a new column and rename the features by index to avoid issues\n",
    "# later when data.frames get transposed and names get checked/changed\n",
    "sacurine$variableMetadata$annotation=rownames(sacurine$variableMetadata)\n",
    "rownames(sacurine$variableMetadata)=1:nrow(sacurine$variableMetadata)\n",
    "colnames(sacurine$dataMatrix)=1:ncol(sacurine$dataMatrix)\n",
    "\n",
    "# create DatasetExperiment\n",
    "DE = DatasetExperiment(data = data.frame(sacurine$dataMatrix),\n",
    "                       sample_meta = sacurine$sampleMetadata,\n",
    "                       variable_meta = sacurine$variableMetadata,\n",
    "                       name = 'Sacurine data',\n",
    "                       description = 'See ropls package documentation for details')\n",
    "\n",
    "# print summary\n",
    "DE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joint-telescope",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "The Sacurine dataset used within this vignette has already been pre-processed:\n",
    "\n",
    "> After signal drift and batch effect correction of intensities, each urine profile was normalized to the osmolality of the sample. Finally, the data were log10 transformed.\n",
    "\n",
    "## Exploratory data analysis\n",
    "Since the data has already been processed the data can be visualised using Principal Component Analysis (PCA) without further pre-processing. The `ropls` package automatically applies unit variance scaling (autoscaling) by default. The same approach is applied here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-delight",
   "metadata": {
    "fig.wide": true,
    "fig.width": 15
   },
   "outputs": [],
   "source": [
    "# prepare model sequence\n",
    "M = autoscale() + PCA(number_components = 5)\n",
    "# apply model sequence to dataset\n",
    "M = model_apply(M,DE)\n",
    "\n",
    "# pca scores plots\n",
    "g=list()\n",
    "for (k in colnames(DE$sample_meta)) {\n",
    "    C = pca_scores_plot(factor_name = k)\n",
    "    g[[k]] = chart_plot(C,M[2])\n",
    "}\n",
    "# plot using cowplot\n",
    "plot_grid(plotlist=g, nrow=1, align='vh', labels=c('A','B','C'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-forestry",
   "metadata": {},
   "source": [
    "The third plot coloured by gender (C) is identical to Figure 2 of the `r Biocpkg(\"ropls\")` package vignette. The `structToolbox` package provides a range of PCA-related diagnostic plots, including D-statistic, scree, and loadings plots. These plots can be used to further explore the variance of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-mountain",
   "metadata": {
    "fig.height": 10,
    "fig.wide": true,
    "fig.width": 9
   },
   "outputs": [],
   "source": [
    "C = pca_scree_plot()\n",
    "g1 = chart_plot(C,M[2])\n",
    "\n",
    "C = pca_loadings_plot()\n",
    "g2 = chart_plot(C,M[2])\n",
    "\n",
    "C = pca_dstat_plot(alpha=0.95)\n",
    "g3 = chart_plot(C,M[2])\n",
    "\n",
    "p1=plot_grid(plotlist = list(g1,g2),align='h',nrow=1,axis='b')\n",
    "p2=plot_grid(plotlist = list(g3),nrow=1)\n",
    "plot_grid(p1,p2,nrow=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-portable",
   "metadata": {},
   "source": [
    "## Partial Least Squares (PLS) analysis\n",
    "The `ropls` package uses its own implementation of the (O)PLS algorithms. `structToolbox` uses the `pls`\n",
    "package, so it is interesting to compare the outputs from both approaches. For simplicity only the scores\n",
    "plots are compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-merit",
   "metadata": {
    "fig.height": 5,
    "fig.width": 5
   },
   "outputs": [],
   "source": [
    "# prepare model sequence\n",
    "M = autoscale() + PLSDA(factor_name='gender')\n",
    "M = model_apply(M,DE)\n",
    "\n",
    "C = plsda_scores_plot(factor_name = 'gender')\n",
    "chart_plot(C,M[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-discharge",
   "metadata": {},
   "source": [
    "The plot is similar to fig.3 of the `r Biocpkg(\"ropls\")` vignette. Differences are due to inverted LV axes, a common occurrence with the NIPALS algorithm (used by both `structToolbox` and `ropls`) which depends on how the algorithm is initialised.\n",
    "\n",
    "To compare the R2 values for the model in structToolbox we have to use a regression model, instead of a discriminant model. For this we convert the gender factor to a numeric variable before applying the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-looking",
   "metadata": {
    "fig.height": 9,
    "fig.wide": true,
    "fig.width": 10
   },
   "outputs": [],
   "source": [
    "# convert gender to numeric\n",
    "DE$sample_meta$gender=as.numeric(DE$sample_meta$gender)\n",
    "\n",
    "# models sequence\n",
    "M = autoscale(mode='both') + PLSR(factor_name='gender',number_components=3)\n",
    "M = model_apply(M,DE)\n",
    "\n",
    "# some diagnostic charts\n",
    "C = plsr_cook_dist()\n",
    "g1 = chart_plot(C,M[2])\n",
    "\n",
    "C = plsr_prediction_plot()\n",
    "g2 = chart_plot(C,M[2])\n",
    "\n",
    "C = plsr_qq_plot()\n",
    "g3 = chart_plot(C,M[2])\n",
    "\n",
    "C = plsr_residual_hist()\n",
    "g4 = chart_plot(C,M[2])\n",
    "\n",
    "plot_grid(plotlist = list(g1,g2,g3,g4), nrow=2,align='vh')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-analysis",
   "metadata": {},
   "source": [
    "The `ropls` package automatically applies cross-validation to asses the performance of the PLSDA model. In `structToolbox` this is applied separately to give more control over the approach used if desired. The default cross-validation used by the `ropls` package is 7-fold cross-validation and we replicate that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-apparatus",
   "metadata": {
    "lines_to_next_cell": 0,
    "results": "asis"
   },
   "outputs": [],
   "source": [
    "# model sequence\n",
    "M = kfold_xval(folds=7, factor_name='gender') * \n",
    "    (autoscale(mode='both') + PLSR(factor_name='gender'))\n",
    "M = run(M,DE,r_squared())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-clock",
   "metadata": {
    "results": "asis",
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "# training set performance\n",
    "cat('Training set R2:\\n')\n",
    "cat(M$metric.train)\n",
    "cat('\\n\\n')\n",
    "# test set performance\n",
    "cat('Test set Q2:\\n')\n",
    "cat(M$metric.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-compact",
   "metadata": {},
   "source": [
    "The validity of the model can further be assessed using permutation testing. For this we will return to a discriminant model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-blind",
   "metadata": {
    "fig.height": 5,
    "fig.width": 5
   },
   "outputs": [],
   "source": [
    "# reset gender to original factor\n",
    "DE$sample_meta$gender=sacurine$sampleMetadata$gender\n",
    "# model sequence\n",
    "M = permutation_test(number_of_permutations = 10, factor_name='gender') *\n",
    "    kfold_xval(folds=7,factor_name='gender') *\n",
    "    (autoscale() + PLSDA(factor_name='gender',number_components = 3))\n",
    "M = run(M,DE,balanced_accuracy())\n",
    "\n",
    "C = permutation_test_plot(style='boxplot')\n",
    "chart_plot(C,M)+ylab('1 - balanced accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-rapid",
   "metadata": {},
   "source": [
    "The permuted models have a balanced accuracy of around 50%, which is to be expected for a dataset with two groups. The unpermuted models have a balanced accuracy of around 90% and is therefore much better than might be expected to occur by chance.\n",
    "\n",
    "# Univariate and multivariate statistical analysis of a NMR-based clinical metabolomics dataset.\n",
    "\n",
    "## Introduction\n",
    "The purpose of this vignette is to demonstrate the different functionalities and methods that are available as part of the structToolbox and reproduce the data analysis reported in [Mendez et al., (2020)](https://link.springer.com/article/10.1007/s11306-019-1588-0) and [Chan et al., (2016)](https://www.nature.com/articles/bjc2015414).\n",
    "\n",
    "## Dataset\n",
    "The 1H-NMR dataset used and described in [Mendez et al., (2020)](https://link.springer.com/article/10.1007/s11306-019-1588-0) and in this vignette contains processed spectra of urine samples obtained from gastric cancer and healthy patients [Chan et al., (2016)](https://www.nature.com/articles/bjc2015414). The experimental raw data is available through Metabolomics Workbench ([PR000699](http://dx.doi.org/10.21228/M8B10B)) and the processed version is available from [here](https://github.com/CIMCB/MetabWorkflowTutorial/raw/master/GastricCancer_NMR.xlsx) as an Excel data file.\n",
    "\n",
    "As a first step we need to reorganise and convert the Excel data file into a DatasetExperiment object. Using the `openxlsx` package the file can be read directly into an R `data.frame` and then manipulated as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/CIMCB/MetabWorkflowTutorial/raw/master/GastricCancer_NMR.xlsx'\n",
    "\n",
    "# read in file directly from github...\n",
    "# X=read.xlsx(url)\n",
    "\n",
    "# ...or use BiocFileCache\n",
    "path = bfcrpath(bfc,url)\n",
    "X = read.xlsx(path)\n",
    "\n",
    "# sample meta data\n",
    "SM=X[,1:4]\n",
    "rownames(SM)=SM$SampleID\n",
    "# convert to factors\n",
    "SM$SampleType=factor(SM$SampleType)\n",
    "SM$Class=factor(SM$Class)\n",
    "# keep a numeric version of class for regression\n",
    "SM$Class_num = as.numeric(SM$Class)\n",
    "\n",
    "## data matrix\n",
    "# remove meta data\n",
    "X[,1:4]=NULL\n",
    "rownames(X)=SM$SampleID\n",
    "\n",
    "# feature meta data\n",
    "VM=data.frame(idx=1:ncol(X))\n",
    "rownames(VM)=colnames(X)\n",
    "\n",
    "# prepare DatasetExperiment\n",
    "DE = DatasetExperiment(\n",
    "    data=X,\n",
    "    sample_meta=SM,\n",
    "    variable_meta=VM,\n",
    "    description='1H-NMR urinary metabolomic profiling for diagnosis of gastric cancer',\n",
    "    name='Gastric cancer (NMR)')\n",
    "\n",
    "DE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-perfume",
   "metadata": {},
   "source": [
    "## Data pre-processing and quality assessment\n",
    "It is good practice to remove any features that may be of low quality, and to assess the quality of the data in general. In the Tutorial features with QC-RSD > 20% and where more than 10% of the features are missing are retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-archive",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# prepare model sequence\n",
    "M = rsd_filter(rsd_threshold=20,qc_label='QC',factor_name='Class') +\n",
    "    mv_feature_filter(threshold = 10,method='across',factor_name='Class')\n",
    "\n",
    "# apply model\n",
    "M = model_apply(M,DE)\n",
    "\n",
    "# get the model output\n",
    "filtered = predicted(M)\n",
    "\n",
    "# summary of filtered data\n",
    "filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-plate",
   "metadata": {},
   "source": [
    "Note there is an additional feature vs the the processing reported by Mendez et. al. because the filters\n",
    "here use >= or <= instead of > and <.\n",
    "\n",
    "After suitable scaling and transformation PCA can be used to assess data quality. It is expected that the biological variance (samples) will be larger than the technical variance (QCs). In the workflow that we are reproducing ([link](https://cimcb.github.io/MetabWorkflowTutorial/Tutorial1.html)) the following steps were applied:\n",
    "\n",
    "- log10 transform\n",
    "- autoscaling (scaled to unit variance)\n",
    "- knn imputation (3 neighbours)\n",
    "\n",
    "The transformed and scaled matrix in then used as input to PCA. Using `struct` we can chain all of these steps into a single model sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-disclaimer",
   "metadata": {
    "fig.height": 5.5,
    "fig.width": 10
   },
   "outputs": [],
   "source": [
    "# prepare the model sequence\n",
    "M = log_transform(base = 10) +\n",
    "    autoscale() + \n",
    "    knn_impute(neighbours = 3) +\n",
    "    PCA(number_components = 10)\n",
    "\n",
    "# apply model sequence to data\n",
    "M = model_apply(M,filtered)\n",
    "\n",
    "# get the transformed, scaled and imputed matrix\n",
    "TSI = predicted(M[3])\n",
    "\n",
    "# scores plot\n",
    "C = pca_scores_plot(factor_name = 'SampleType')\n",
    "g1 = chart_plot(C,M[4])\n",
    "\n",
    "# loadings plot\n",
    "C = pca_loadings_plot()\n",
    "g2 = chart_plot(C,M[4])\n",
    "\n",
    "plot_grid(g1,g2,align='hv',nrow=1,axis='tblr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-relaxation",
   "metadata": {},
   "source": [
    "## Univariate statistics\n",
    "`structToolbox` provides a number of objects for ttest, counting numbers of features etc. For brevity only the ttest is calculated for comparison with the workflow we are following ([link](https://cimcb.github.io/MetabWorkflowTutorial/Tutorial1.html)). The QC samples need to be excluded, and the data reduced to only the GC and HE groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-miami",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model\n",
    "TT = filter_smeta(mode='include',factor_name='Class',levels=c('GC','HE')) +  \n",
    "     ttest(alpha=0.05,mtc='fdr',factor_names='Class')\n",
    "\n",
    "# apply model\n",
    "TT = model_apply(TT,filtered)\n",
    "\n",
    "# keep the data filtered by group for later\n",
    "filtered = predicted(TT[1])\n",
    "\n",
    "# convert to data frame\n",
    "out=as_data_frame(TT[2])\n",
    "\n",
    "# show first few features\n",
    "head(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-slope",
   "metadata": {},
   "source": [
    "##  Multivariate statistics and machine learning\n",
    "\n",
    "### Training and Test sets\n",
    "Splitting data into training and test sets is an important aspect of machine learning. In `structToolbox` this is implemented using the `split_data` object for random subsampling across the whole dataset, and `stratified_split` for splitting based on group sizes, which is the approach used by Mendez et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-adaptation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model\n",
    "M = stratified_split(p_train=0.75,factor_name='Class')\n",
    "# apply to filtered data\n",
    "M = model_apply(M,filtered)\n",
    "# get data from object\n",
    "train = M$training\n",
    "train\n",
    "cat('\\n')\n",
    "\n",
    "test = M$testing\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-death",
   "metadata": {},
   "source": [
    "### Optimal number of PLS components\n",
    "In Mendez et al a k-fold cross-validation is used to determine the optimal number of PLS components. 100 bootstrap iterations are used to generate confidence intervals. In `strucToolbox` these are implemented using \"iterator\" objects, that can be combined with model objects. R2 is used as the metric for optimisation, so the `PLSR` model in structToolbox will be used. For speed only 10 bootstrap iterations are used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale/transform training data\n",
    "M = log_transform(base = 10) +\n",
    "    autoscale() + \n",
    "    knn_impute(neighbours = 3,by='samples')\n",
    "\n",
    "# apply model\n",
    "M = model_apply(M,train)\n",
    "\n",
    "# get scaled/transformed training data\n",
    "train_st = predicted(M)\n",
    "\n",
    "# prepare model sequence\n",
    "MS = grid_search_1d(\n",
    "        param_to_optimise = 'number_components',\n",
    "        search_values = as.numeric(c(1:6)),\n",
    "        model_index = 2,\n",
    "        factor_name = 'Class_num',\n",
    "        max_min = 'max') *\n",
    "     permute_sample_order(\n",
    "        number_of_permutations = 10) *\n",
    "     kfold_xval(\n",
    "        folds = 5,\n",
    "        factor_name = 'Class_num') * \n",
    "     (mean_centre(mode='sample_meta')+\n",
    "      PLSR(factor_name='Class_num'))\n",
    "\n",
    "# run the validation\n",
    "MS = struct::run(MS,train_st,r_squared())\n",
    "\n",
    "#\n",
    "C = gs_line()\n",
    "chart_plot(C,MS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-palestinian",
   "metadata": {},
   "source": [
    "The chart plotted shows Q2, which is comparable with Figure 13 of [Mendez et al]((https://cimcb.github.io/MetabWorkflowTutorial/Tutorial1.html)) . Two components were selected by Mendez et al, so we will use that here.\n",
    "\n",
    "### PLS model evalutation\n",
    "To evaluate the model for discriminant analysis in structToolbox the `PLSDA` model is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-reviewer",
   "metadata": {
    "fig.height": 5.5,
    "fig.wide": true,
    "fig.width": 15,
    "lines_to_next_cell": 0,
    "warning": false
   },
   "outputs": [],
   "source": [
    "# prepare the discriminant model\n",
    "P = PLSDA(number_components = 2, factor_name='Class')\n",
    "\n",
    "# apply the model\n",
    "P = model_apply(P,train_st)\n",
    "\n",
    "# charts\n",
    "C = plsda_predicted_plot(factor_name='Class',style='boxplot')\n",
    "g1 = chart_plot(C,P)\n",
    "\n",
    "C = plsda_predicted_plot(factor_name='Class',style='density')\n",
    "g2 = chart_plot(C,P)+xlim(c(-2,2))\n",
    "\n",
    "C = plsda_roc_plot(factor_name='Class')\n",
    "g3 = chart_plot(C,P)\n",
    "\n",
    "plot_grid(g1,g2,g3,align='vh',axis='tblr',nrow=1, labels=c('A','B','C'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-projection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC for comparison with Mendez et al\n",
    "MET = calculate(AUC(),P$y$Class,P$yhat[,1])\n",
    "MET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-hometown",
   "metadata": {},
   "source": [
    "Note that the default cutoff in A and B of the figure above for the PLS models in `structToolbox` is 0, because groups are encoded as +/-1. This has no impact on the overall performance of the model.\n",
    "\n",
    "### Permutation test\n",
    "A permutation test can be used to assess how likely the observed result is to have occurred by chance. In structToolbox `permutation_test` is an iterator object that can be combined with other iterators and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model sequence\n",
    "MS = permutation_test(number_of_permutations = 20,factor_name = 'Class_num') * \n",
    "     kfold_xval(folds = 5,factor_name = 'Class_num') *\n",
    "     (mean_centre(mode='sample_meta') + PLSR(factor_name='Class_num', number_components = 2))\n",
    "\n",
    "# run iterator\n",
    "MS = struct::run(MS,train_st,r_squared())\n",
    "\n",
    "# chart\n",
    "C = permutation_test_plot(style = 'density') \n",
    "chart_plot(C,MS) + xlim(c(-1,1)) + xlab('R Squared')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-acquisition",
   "metadata": {},
   "source": [
    "This plot is comparable to the bottom half of Figure 17 in [Mendez et. al.](https://cimcb.github.io/MetabWorkflowTutorial/Tutorial1.html). The unpermuted (true) Q2 values are consistently better than the permuted (null) models. i.e. the model is reliable.\n",
    "\n",
    "### PLS projection plots\n",
    "PLS can also be used to visualise the model and interpret the latent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-feedback",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the discriminant model\n",
    "P = PLSDA(number_components = 2, factor_name='Class')\n",
    "\n",
    "# apply the model\n",
    "P = model_apply(P,train_st)\n",
    "\n",
    "C = plsda_scores_plot(components=c(1,2),factor_name = 'Class')\n",
    "chart_plot(C,P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-minister",
   "metadata": {},
   "source": [
    "### PLS feature importance\n",
    "Regression coefficients and VIP scores can be used to estimate the importance of individual features to the PLS model. In Mendez et al bootstrapping is used to estimate the confidence intervals, but for brevity here we will skip this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-dominican",
   "metadata": {
    "fig.height": 5.5,
    "fig.width": 10
   },
   "outputs": [],
   "source": [
    "# prepare chart\n",
    "C = plsda_vip_plot(level='HE')\n",
    "g1 = chart_plot(C,P)\n",
    "\n",
    "C = plsda_regcoeff_plot(level='HE')\n",
    "g2 = chart_plot(C,P)\n",
    "\n",
    "plot_grid(g1,g2,align='hv',axis='tblr',nrow=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-jackson",
   "metadata": {},
   "source": [
    "# Classification of Metabolomics Data using Support Vector Machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-former",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "knitr::opts_chunk$set(echo = TRUE,fig.wide=TRUE,fig.width=5,fig.height=5.5)\n",
    "set.seed(57475)\n",
    "\n",
    "# read in file using BiocFileCache\n",
    "url='https://github.com/CIMCB/MetabWorkflowTutorial/raw/master/GastricCancer_NMR.xlsx'\n",
    "path = bfcrpath(bfc,url)\n",
    "X = read.xlsx(path)\n",
    "\n",
    "# sample meta data\n",
    "SM=X[,1:4]\n",
    "rownames(SM)=SM$SampleID\n",
    "# convert to factors\n",
    "SM$SampleType=factor(SM$SampleType)\n",
    "SM$Class=factor(SM$Class)\n",
    "# keep a numeric version of class for regression\n",
    "SM$Class_num = as.numeric(SM$Class)\n",
    "\n",
    "## data matrix\n",
    "# remove meta data\n",
    "X[,1:4]=NULL\n",
    "rownames(X)=SM$SampleID\n",
    "\n",
    "# feature meta data\n",
    "VM=data.frame(idx=1:ncol(X))\n",
    "rownames(VM)=colnames(X)\n",
    "\n",
    "# prepare DatasetExperiment\n",
    "DE = DatasetExperiment(\n",
    "    data=X,\n",
    "    sample_meta=SM,\n",
    "    variable_meta=VM,\n",
    "    description='1H-NMR urinary metabolomic profiling for diagnosis of gastric cancer',\n",
    "    name='Gastric cancer (NMR)')\n",
    "\n",
    "M = rsd_filter(rsd_threshold=20,qc_label='QC',factor_name='Class') +\n",
    "    mv_feature_filter(threshold = 10,method='across',factor_name='Class') +\n",
    "    log_transform(base = 10) +\n",
    "    autoscale() + \n",
    "    knn_impute(neighbours = 3)\n",
    "\n",
    "M = model_apply(M,DE)\n",
    "DE = predicted(M)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-ordinance",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "The aim of this vignette is to illustrate how to apply SVM analysis for Classifying Metabolomics data.\n",
    "\n",
    "Support vector Machines (SVM) are a commonly used method in Machine Learning. For classification tasks they are used to generate a boundary between groups of samples in the training set. As well as generating linear boundaries, SVM can be extended to exploit the use of kernels and generate complex non-linear boundaries between groups if required.\n",
    "\n",
    "For the `structToolbox` package, SVM functionality provided by the `r CRANpkg(\"e1071\")` package has been incorporated into a `model` object. A chart object (`svm_plot_2d`) is also available to plot SVM boundaries for data with two variables.\n",
    "\n",
    "## Dataset\n",
    "The 1H-NMR dataset used and described in [Mendez et al., (2020)](https://link.springer.com/article/10.1007/s11306-019-1588-0, https://github.com/CIMCB/MetabWorkflowTutorial) and in this vignette contains processed spectra of urine samples obtained from gastric cancer and healthy patients [Chan et al., (2016)](https://www.nature.com/articles/bjc2015414). The raw experimental data is available through Metabolomics Workbench ([PR000699](http://dx.doi.org/10.21228/M8B10B)) and the processed version is available from [here](https://github.com/CIMCB/MetabWorkflowTutorial/raw/master/GastricCancer_NMR.xlsx) as an Excel data file.\n",
    "\n",
    "For simplicity we will use a pre-processed version of the 1H-NMR \"Gastric cancer\" dataset using the `structToolbox` package. Details in regards to pre-processing are reported in the \"NMR_clinical_metabolomics\" vignette of the `r Biocpkg(\"structToolbox\") package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-romantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of DatasetExperiment object\n",
    "DE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-tribe",
   "metadata": {},
   "source": [
    "For the purposes of illustrating the effect of SVM parameters on the boundary between groups, we reduce the data to include only the GC and HE groups and apply PLS to reduce the data to two components. We then treat the PLS scores as as a two group dataset with only two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model sequence and pls model (NB data already centred)\n",
    "MS = filter_smeta(mode = 'include', levels = c('GC','HE'), factor_name = 'Class') +\n",
    "     PLSDA(factor_name = 'Class',number_components = 2)\n",
    "\n",
    "# apply PLS model\n",
    "MS = model_apply(MS,DE)\n",
    "\n",
    "# plot the data\n",
    "C = plsda_scores_plot(factor_name = 'Class')\n",
    "chart_plot(C,MS[2])\n",
    "\n",
    "# new DatasetExperiment object from the PLS scores\n",
    "DE2 = DatasetExperiment(\n",
    "  data = MS[2]$scores, \n",
    "  sample_meta = predicted(MS[1])$sample_meta,\n",
    "  variable_meta = data.frame('LV'=c(1,2),row.names = colnames(MS[2]$scores)),\n",
    "  name = 'Illustrativate SVM dataset',\n",
    "  description = 'Generated by applying PLS to the processed Gastric cancer (NMR) dataset'\n",
    ")\n",
    "\n",
    "DE2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-accident",
   "metadata": {},
   "source": [
    "## Basic SVM model\n",
    "The simplest SVM model uses a linear kernel. In `structToolbox` the `SVM` model can be used to train and apply SVM models. A `svm_plot_2d` chart object is provided for visualisation of boundaries in two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-chancellor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM model\n",
    "M = SVM(\n",
    "  factor_name = 'Class',\n",
    "  kernel = 'linear'\n",
    ")\n",
    "\n",
    "# apply model\n",
    "M = model_apply(M,DE2)\n",
    "\n",
    "# plot boundary\n",
    "C = svm_plot_2d(factor_name = 'Class')\n",
    "chart_plot(C,M, DE2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "derived-professor",
   "metadata": {},
   "source": [
    "The SVM boundary is plotted in black, the margins in grey and support vectors are indicated by grey circles.\n",
    "\n",
    "## SVM cost function\n",
    "The SVM cost function applies a penalty to samples on the wrong side of the margins. A high penalty results\n",
    "in a narrow margin and tries to force more samples to be on the correct side of the boundary. A low penalty\n",
    "makes for a wider margin and is less strict about samples being misclassified. The optimal cost to use is data dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-ottawa",
   "metadata": {
    "fig.height": 11,
    "fig.width": 10
   },
   "outputs": [],
   "source": [
    "# low cost\n",
    "M$cost=0.01\n",
    "M=model_apply(M,DE2)\n",
    "C=svm_plot_2d(factor_name='Species')\n",
    "g1=chart_plot(C,M,DE2)\n",
    "\n",
    "# medium cost\n",
    "M$cost=0.05\n",
    "M=model_apply(M,DE2)\n",
    "C=svm_plot_2d(factor_name='Species')\n",
    "g2=chart_plot(C,M,DE2)\n",
    "\n",
    "# high cost\n",
    "M$cost=100\n",
    "M=model_apply(M,DE2)\n",
    "\n",
    "C=svm_plot_2d(factor_name='Species')\n",
    "g3=chart_plot(C,M,DE2)\n",
    "\n",
    "# plot\n",
    "prow <- plot_grid(\n",
    "  g1 + theme(legend.position=\"none\"),\n",
    "  g2 + theme(legend.position=\"none\"),\n",
    "  g3 + theme(legend.position=\"none\"),\n",
    "  align = 'vh',\n",
    "  labels = c(\"Low cost\", \"Medium cost\", \"High cost\"),\n",
    "  hjust = -1,\n",
    "  nrow = 2\n",
    ")\n",
    "\n",
    "legend <- get_legend(\n",
    "  # create some space to the left of the legend\n",
    "  g1 + guides(color = guide_legend(nrow = 1)) +\n",
    "  theme(legend.position = \"bottom\")\n",
    ")\n",
    "\n",
    "plot_grid(prow, legend, ncol=1, rel_heights = c(1, .1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-thursday",
   "metadata": {},
   "source": [
    "## Kernel functions\n",
    "A number of different kernels can be used with support vector machines. For the `structToolbox` wrapper\n",
    "linear, polynomial,radial and sigmoid kernels can be specified. Using kernels allows the boundary to be\n",
    "more flexible, but often require additional parameters to be specified. The best kernel to use will vary depending on the dataset, but a common choice is the radial kernel as it allows high flexibility with a single parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-admission",
   "metadata": {
    "fig.height": 11,
    "fig.width": 10
   },
   "outputs": [],
   "source": [
    "# set a fixed cost for this comparison\n",
    "M$cost=1\n",
    "\n",
    "# linear kernel\n",
    "M$kernel='linear'\n",
    "M=model_apply(M,DE2)\n",
    "C=svm_plot_2d(factor_name='Species')\n",
    "g1=chart_plot(C,M,DE2)\n",
    "\n",
    "# polynomial kernel\n",
    "M$kernel='polynomial'\n",
    "M$gamma=1\n",
    "M$coef0=0\n",
    "M=model_apply(M,DE2)\n",
    "C=svm_plot_2d(factor_name='Species')\n",
    "g2=chart_plot(C,M,DE2)\n",
    "\n",
    "# rbf kernel\n",
    "M$kernel='radial'\n",
    "M$gamma=1\n",
    "M=model_apply(M,DE2)\n",
    "C=svm_plot_2d(factor_name='Species')\n",
    "g3=chart_plot(C,M,DE2)\n",
    "\n",
    "# sigmoid kernel\n",
    "M$kernel='sigmoid'\n",
    "M$gamma=1\n",
    "M$coef0=0\n",
    "M=model_apply(M,DE2)\n",
    "C=svm_plot_2d(factor_name='Species')\n",
    "g4=chart_plot(C,M,DE2)\n",
    "\n",
    "# plot\n",
    "prow <- plot_grid(\n",
    "  g1 + theme(legend.position=\"none\"),\n",
    "  g2 + theme(legend.position=\"none\"),\n",
    "  g3 + theme(legend.position=\"none\"),\n",
    "  g4 + theme(legend.position=\"none\"),\n",
    "  align = 'vh',\n",
    "  labels = c(\"Linear\", \"Polynomial\", \"Radial\",\"Sigmoid\"),\n",
    "  hjust = -1,\n",
    "  nrow = 2\n",
    ")\n",
    "legend <- get_legend(\n",
    "  # create some space to the left of the legend\n",
    "  g1 + guides(color = guide_legend(nrow = 1)) +\n",
    "    theme(legend.position = \"bottom\")\n",
    ")\n",
    "plot_grid(prow, legend, ncol = 1, rel_heights = c(1, .1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-stranger",
   "metadata": {},
   "source": [
    "The parameters of a kernel can be used to control the complexity of the boundary. Here I show how the\n",
    "radial kernel parameter gamma can be used to change the complexity of the boundary. In combination\n",
    "with the cost parameter (which I keep constant here) this allows for highly flexible boundary models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-egypt",
   "metadata": {
    "fig.height": 11,
    "fig.width": 10
   },
   "outputs": [],
   "source": [
    "# rbf kernel and cost\n",
    "M$kernel = 'radial'\n",
    "M$cost = 1\n",
    "\n",
    "# low gamma\n",
    "M$gamma=0.01\n",
    "M=model_apply(M,DE2)\n",
    "C=svm_plot_2d(factor_name='Species')\n",
    "g1=chart_plot(C,M,DE2)\n",
    "\n",
    "# medium gamma\n",
    "M$gamma=0.1\n",
    "M=model_apply(M,DE2)\n",
    "C=svm_plot_2d(factor_name='Species')\n",
    "g2=chart_plot(C,M,DE2)\n",
    "\n",
    "# high gamma\n",
    "M$gamma=1\n",
    "M=model_apply(M,DE2)\n",
    "C=svm_plot_2d(factor_name='Species')\n",
    "g3=chart_plot(C,M,DE2)\n",
    "\n",
    "# plot\n",
    "prow <- plot_grid(\n",
    " g1 + theme(legend.position=\"none\"),\n",
    " g2 + theme(legend.position=\"none\"),\n",
    " g3 + theme(legend.position=\"none\"),\n",
    " align = 'vh',\n",
    " labels = c(\"Low gamma\", \"Medium gamma\", \"High gamma\"),\n",
    " hjust = -1,\n",
    " nrow = 2\n",
    ")\n",
    "legend <- get_legend(\n",
    "  # create some space to the left of the legend\n",
    "  g1 + guides(color = guide_legend(nrow = 1)) +\n",
    "  theme(legend.position = \"bottom\")\n",
    ")\n",
    "plot_grid(prow, legend, ncol = 1, rel_heights = c(1, .1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-viking",
   "metadata": {},
   "source": [
    "Note that best practice would be to select the optimal kernel parameter(s) in combination with the cost parameter (e.g. by 2d grid search) so that the best combination of both is identified.\n",
    "\n",
    "# Exploratory data analysis of LC-MS-based proteomics and metabolomics datasets (STATegra project)\n",
    "\n",
    "## Introduction\n",
    "The aim of this vignette is to conduct data preprocessing and exploratory analysis of data from the STATegra project (https://www.nature.com/articles/s41597-019-0202-7). For demonstration purposes we will focus on the Proteomics and Metabolomics datasets that are publicly available as part of the STATegra multi-omics dataset. \n",
    "\n",
    ">...the STATegra multi-omics dataset combines measurements from up to 10 different omics technologies applied to the same biological system, namely the well-studied mouse pre-B-cell differentiation. STATegra includes high-throughput measurements of chromatin structure, gene expression, proteomics and metabolomics, and it is complemented with single-cell data. \n",
    "[Gomez-Cabrero et al](https://www.nature.com/articles/s41597-019-0202-7)\n",
    "\n",
    "STATegra includes high-throughput measurements of chromatin structure, gene expression, proteomics and metabolomics, and it is complemented with single-cell data.\n",
    "\n",
    "## LC-MS-based proteomics dataset\n",
    "The LC-MS-based proteomics dataset from the STATegra multi-omics dataset (see Introduction) can be found on [github](https://github.com/STATegraData/STATegraData) and must be extracted from the zip file prior to data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-consultancy",
   "metadata": {
    "message": false,
    "warning": false
   },
   "outputs": [],
   "source": [
    "# path to zip\n",
    "zipfile = \"https://raw.github.com/STATegraData/STATegraData/master/Script_STATegra_Proteomics.zip\"\n",
    "\n",
    "## retrieve from BiocFileCache\n",
    "path = bfcrpath(bfc,zipfile)\n",
    "temp = bfccache(bfc)\n",
    "\n",
    "## ... or download to temp location\n",
    "# path = tempfile()\n",
    "# temp = tempdir()\n",
    "# download.file(zipfile,path)\n",
    "\n",
    "# unzip\n",
    "unzip(path, files = \"Proteomics_01_uniprot_canonical_normalized.txt\", exdir=temp)\n",
    "# read samples\n",
    "all_data <-  read.delim(file.path(temp,\"Proteomics_01_uniprot_canonical_normalized.txt\"), as.is = TRUE, header = TRUE, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-lawyer",
   "metadata": {},
   "source": [
    "The imported data needs to be converted to `DatasetExperiment` format for use with `structToolbox`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data matrix\n",
    "data = all_data[1:2527,51:86]\n",
    "# shorten sample names\n",
    "colnames(data) = lapply(colnames(data), function (x) substr(x, 27, nchar(x)))\n",
    "# replace 0 with NA\n",
    "data[data == 0] <- NA\n",
    "# transpose\n",
    "data=as.data.frame(t(data))\n",
    "\n",
    "# prepare sample meta\n",
    "SM = lapply(rownames(data),function(x) {\n",
    "  s=strsplit(x,'_')[[1]] # split at underscore\n",
    "  out=data.frame(\n",
    "    'treatment' = s[[1]],\n",
    "    'time' = substr(s[[2]],1,nchar(s[[2]])-1) ,    \n",
    "    'batch' = substr(s[[3]],6,nchar(s[[3]])),\n",
    "    'condition' = substr(x,1,6) # interaction between treatment and time\n",
    "  )\n",
    "  return(out)\n",
    "})\n",
    "SM = do.call(rbind,SM)\n",
    "rownames(SM)=rownames(data)\n",
    "# convert to factors\n",
    "SM$treatment=factor(SM$treatment)\n",
    "SM$time=ordered(SM$time,c(\"0\",\"2\",\"6\",\"12\",\"18\",\"24\"))\n",
    "SM$batch=ordered(SM$batch,c(1,3,4,5,6,7))\n",
    "SM$condition=factor(SM$condition)\n",
    "\n",
    "# variable meta data\n",
    "VM = all_data[1:2527,c(1,6,7)]\n",
    "rownames(VM)=colnames(data)\n",
    "\n",
    "# prepare DatasetExperiment\n",
    "DS = DatasetExperiment(\n",
    "  data = data, \n",
    "  sample_meta = SM, \n",
    "  variable_meta = VM, \n",
    "  name = 'STATegra Proteomics',\n",
    "  description = 'downloaded from: https://github.com/STATegraData/STATegraData/'\n",
    ")\n",
    "DS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-turner",
   "metadata": {},
   "source": [
    "A number of Reporter genes  were included in the study. We plot two of them here to illustrate some trends in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-recall",
   "metadata": {
    "fig.height": 4.5,
    "fig.width": 10
   },
   "outputs": [],
   "source": [
    "# find id of reporters\n",
    "Ldha = which(DS$variable_meta$Gene.names=='Ldha')\n",
    "Hk2 = which(DS$variable_meta$Gene.names=='Hk2')\n",
    "\n",
    "# chart object\n",
    "C = feature_boxplot(feature_to_plot=Ldha,factor_name='time',label_outliers=FALSE)\n",
    "g1=chart_plot(C,DS)+ggtitle('Ldha')+ylab('expression')\n",
    "C = feature_boxplot(feature_to_plot=Hk2,factor_name='time',label_outliers=FALSE)\n",
    "g2=chart_plot(C,DS)+ggtitle('Hk2')+ylab('expression')\n",
    "\n",
    "plot_grid(g1,g2,nrow=1,align='vh',axis='tblr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-upper",
   "metadata": {},
   "source": [
    "### Data transformation\n",
    "The data is log2 transformed, then scaled such that the mean of the medians is equal for all conditions. These steps are available in `structToolbox` using `log_transform` and `mean_of_medians` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-individual",
   "metadata": {
    "fig.height": 4.5,
    "fig.width": 10
   },
   "outputs": [],
   "source": [
    "# prepare model sequence\n",
    "M = log_transform(\n",
    "  base=2) + \n",
    "  mean_of_medians(\n",
    "    factor_name = 'condition')\n",
    "# apply model sequence\n",
    "M = model_apply(M,DS) \n",
    "\n",
    "# get transformed data\n",
    "DST = predicted(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-audio",
   "metadata": {},
   "source": [
    "The Reporter genes are plotted again for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-college",
   "metadata": {
    "fig.height": 4.5,
    "fig.width": 10
   },
   "outputs": [],
   "source": [
    "# chart object\n",
    "C = feature_boxplot(feature_to_plot=Ldha,factor_name='time',label_outliers=FALSE)\n",
    "g1=chart_plot(C,DST)+ggtitle('Ldha')+ylab('log2(expression)')\n",
    "C = feature_boxplot(feature_to_plot=Hk2,factor_name='time',label_outliers=FALSE)\n",
    "g2=chart_plot(C,DST)+ggtitle('Hk2')+ylab('log2(expression)')\n",
    "\n",
    "plot_grid(g1,g2,nrow=1,align='vh',axis='tblr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-medline",
   "metadata": {},
   "source": [
    "### Missing value filtering\n",
    "Missing value filtering involves removing any feature (gene) where there are at least 3 missing values per group in at least 11 samples.\n",
    "\n",
    "This specific filter is not in `structToolbox` at this time, but can be achieved by combining `filter_na_count` and `filter_by_name` objects. \n",
    "\n",
    "Specifically, the default output of `filter_na_count` is changed to return a matrix of NA counts per class. This output is then connected to the 'names' input of `filter_by_names` and converted to TRUE/FALSE using the 'seq_fcn' input. \n",
    "\n",
    "The 'seq_fcn' function processes the NA counts _before_ they are used as inputs for `filter_by_names`. When data is passed along the model sequence it passes unchanged through the `filter_na_count` object becuase the default output has been changed, so the `filter_na_count` and `filter_by_name` objects are working together as a single filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model sequence\n",
    "M2 = filter_na_count(\n",
    "  threshold=2,\n",
    "  factor_name='condition', \n",
    "  predicted='na_count') +    # override the default output\n",
    "  filter_by_name(\n",
    "    mode='exclude',\n",
    "    dimension='variable',\n",
    "    names='place_holder',\n",
    "    seq_in='names',\n",
    "    seq_fcn=function(x) {      # convert NA count pre group to true/false\n",
    "      x=x>2                  # more the two missing per group\n",
    "      x=rowSums(x)>10        # in more than 10 groups\n",
    "      return(x)\n",
    "    }\n",
    "  )\n",
    "# apply to transformed data\n",
    "M2 = model_apply(M2,DST)\n",
    "\n",
    "# get the filtered data\n",
    "DSTF = predicted(M2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-series",
   "metadata": {},
   "source": [
    "### Missing value imputation\n",
    "STATegra uses two imputation methods that are not available as `struct` objects, so we create temporary `STATegra_impute` objects to do this using some functions from the `struct` package.\n",
    "\n",
    "The first imputation method imputes missing values for any treatment where values are missing for all samples using a \"random value below discovery\". We create a new struct object using `set_struct_obj` in the global environment, and a \"method_apply\" method that implements the imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new imputation object\n",
    "set_struct_obj(\n",
    "  class_name = 'STATegra_impute1',\n",
    "  struct_obj = 'model',\n",
    "  stato=FALSE,\n",
    "  params=c(factor_sd='character',factor_name='character'),\n",
    "  outputs=c(imputed='DatasetExperiment'),\n",
    "  prototype = list(\n",
    "    name = 'STATegra imputation 1',\n",
    "    description = 'If missing values are present for all one group then they are replaced with min/2 + \"random value below discovery\".',\n",
    "    predicted = 'imputed'\n",
    "  ) \n",
    ")\n",
    "\n",
    "# create method_apply for imputation method 1\n",
    "set_obj_method(\n",
    "  class_name='STATegra_impute1',\n",
    "  method_name='model_apply',\n",
    "  definition=function(M,D) {\n",
    "    \n",
    "    # for each feature count NA within each level\n",
    "    na = apply(D$data,2,function(x){\n",
    "      tapply(x,D$sample_meta[[M$factor_name]],function(y){\n",
    "        sum(is.na(y))\n",
    "      })\n",
    "    })\n",
    "    # count number of samples in each group\n",
    "    count=summary(D$sample_meta[[M$factor_name]])\n",
    "    # standard deviation of features within levels of factor_sd\n",
    "    sd = apply(D$data,2,function(x) {tapply(x,D$sample_meta[[M$factor_sd]],sd,na.rm=TRUE)})\n",
    "    sd = median(sd,na.rm=TRUE)\n",
    "    \n",
    "    # impute or not\n",
    "    check=na == matrix(count,nrow=2,ncol=ncol(D)) # all missing in one class\n",
    "    \n",
    "    # impute matrix\n",
    "    mi = D$data\n",
    "    for (j in 1:nrow(mi)) {\n",
    "      # index of group for this sample\n",
    "      g = which(levels(D$sample_meta[[M$factor_name]])==D$sample_meta[[M$factor_name]][j])\n",
    "      iv=rnorm(ncol(D),min(D$data[j,],na.rm=TRUE)/2,sd)\n",
    "      mi[j,is.na(mi[j,]) & check[g,]] = iv[is.na(mi[j,]) & check[g,]]\n",
    "    }\n",
    "    D$data = mi\n",
    "    M$imputed=D\n",
    "    return(M)\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-utilization",
   "metadata": {},
   "source": [
    "The second imputation method replacing missing values in any condition with exactly 1 missing value with the mean of the values for that condition. Again we create a new struct object and corresponding method for the the new object to implement the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new imputation object\n",
    "set_struct_obj(\n",
    "  class_name = 'STATegra_impute2',\n",
    "  struct_obj = 'model',\n",
    "  stato=FALSE,\n",
    "  params=c(factor_name='character'),\n",
    "  outputs=c(imputed='DatasetExperiment'),\n",
    "  prototype = list(\n",
    "    name = 'STATegra imputation 2',\n",
    "    description = 'For those conditions with only 1 NA impute with the mean of the condition.',\n",
    "    predicted = 'imputed'\n",
    "  ) \n",
    ")\n",
    "\n",
    "# create method_apply for imputation method 2\n",
    "set_obj_method(\n",
    "  class_name='STATegra_impute2',\n",
    "  method_name='model_apply',\n",
    "  definition=function(M,D) {\n",
    "    # levels in condition\n",
    "    L = levels(D$sample_meta[[M$factor_name]])\n",
    "    \n",
    "    # for each feature count NA within each level\n",
    "    na = apply(D$data,2,function(x){\n",
    "      tapply(x,D$sample_meta[[M$factor_name]],function(y){\n",
    "        sum(is.na(y))\n",
    "      })\n",
    "    })\n",
    "    \n",
    "    # standard deviation of features within levels of factor_sd\n",
    "    sd = apply(D$data,2,function(x) {tapply(x,D$sample_meta[[M$factor_name]],sd,na.rm=TRUE)})\n",
    "    sd = median(sd,na.rm=TRUE)\n",
    "    \n",
    "    # impute or not\n",
    "    check=na == 1 # only one missing for a condition\n",
    "    \n",
    "    # index of samples for each condition\n",
    "    IDX = list()\n",
    "    for (k in L) {\n",
    "      IDX[[k]]=which(D$sample_meta[[M$factor_name]]==k)\n",
    "    }\n",
    "    \n",
    "    ## impute\n",
    "    # for each feature\n",
    "    for (k in 1:ncol(D)) {\n",
    "      # for each condition\n",
    "      for (j in 1:length(L)) {\n",
    "        # if passes test\n",
    "        if (check[j,k]) {\n",
    "          # mean of samples in group\n",
    "          m = mean(D$data[IDX[[j]],k],na.rm=TRUE)\n",
    "          # imputed value\n",
    "          im = rnorm(1,m,sd)\n",
    "          # replace NA with imputed\n",
    "          D$data[is.na(D$data[,k]) & D$sample_meta[[M$factor_name]]==L[j],k]=im\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    M$imputed=D\n",
    "    return(M)\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-costs",
   "metadata": {},
   "source": [
    "The new STATegra imputation objects can now be used in model sequences like any other `struct` object. A final filter is added to remove any feature that has missing values after imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-sleeve",
   "metadata": {
    "fig.height": 4.5,
    "fig.width": 10
   },
   "outputs": [],
   "source": [
    "# model sequence\n",
    "M3 = STATegra_impute1(factor_name='treatment',factor_sd='condition') +\n",
    "     STATegra_impute2(factor_name = 'condition') + \n",
    "     filter_na_count(threshold = 3, factor_name='condition')\n",
    "# apply model\n",
    "M3 = model_apply(M3,DSTF)\n",
    "# get imputed data\n",
    "DSTFI = predicted(M3)\n",
    "DSTFI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-gospel",
   "metadata": {},
   "source": [
    "### Exploratory analysis\n",
    "It is often useful to visualise the distribution of values across samples to verify that the transformations/normalisation/filtering etc have been effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-geometry",
   "metadata": {
    "fig.height": 10,
    "fig.width": 9,
    "message": false,
    "tags": [
     "remove_input"
    ],
    "warning": false
   },
   "outputs": [],
   "source": [
    "# distributions plot\n",
    "C = compare_dist(factor_name = 'treatment')\n",
    "g=chart_plot(C,DS,DSTFI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-wells",
   "metadata": {},
   "source": [
    "The values are no longer skewed and show an approximately normal distribution. The boxplots are comparable in width with very few outliers indicated, so the transformations etc have had an overall positive effect.\n",
    "\n",
    "PCA is used to provide a graphical representation of the data. For comparison with the outputs from STATegra a filter is included to reduce the data to include only the treated samples (IKA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-running",
   "metadata": {
    "fig.height": 5,
    "fig.width": 9,
    "message": false,
    "warning": false
   },
   "outputs": [],
   "source": [
    "# model sequence\n",
    "P = filter_smeta(mode='include',factor_name='treatment',levels='IKA') +\n",
    "    mean_centre() + \n",
    "    PCA(number_components = 2)\n",
    "# apply model\n",
    "P = model_apply(P,DSTFI)\n",
    "\n",
    "# scores plots coloured by factors\n",
    "g = list()\n",
    "for (k in c('batch','time')) {\n",
    "  C = pca_scores_plot(factor_name=k,ellipse='none')\n",
    "  g[[k]]=chart_plot(C,P[3])\n",
    "}\n",
    "\n",
    "plot_grid(plotlist = g,nrow=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-serial",
   "metadata": {},
   "source": [
    "There does not appear to be a strong batch effect. PC1 is dominated by time point \"24\" and some potentially outlying points from time points \"2\" and \"0\".\n",
    "\n",
    "## LC-MS-based metabolomics dataset\n",
    "The LC-MS-based metabolomics dataset from the STATegra multi-omics dataset (see Introduction) can be found on [github](https://github.com/STATegraData/STATegraData) and must be extracted from zip file prior to data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-canada",
   "metadata": {
    "lines_to_next_cell": 0,
    "message": false,
    "warning": false
   },
   "outputs": [],
   "source": [
    "# path to zip\n",
    "zipfile = \"https://raw.github.com/STATegraData/STATegraData/master/Script_STATegra_Metabolomics.zip\"\n",
    "\n",
    "## retrieve from BiocFileCache\n",
    "path = bfcrpath(bfc,zipfile)\n",
    "temp = bfccache(bfc)\n",
    "\n",
    "## ... or download to temp location\n",
    "# path = tempfile()\n",
    "# temp = tempdir()\n",
    "# download.file(zipfile,path)\n",
    "\n",
    "# unzip\n",
    "unzip(zipfile=path, files = \"LC_MS_raw_data.xlsx\", exdir=temp)\n",
    "\n",
    "# read samples\n",
    "data <- as.data.frame(read.xlsx(file.path(temp,\"LC_MS_raw_data.xlsx\"),sheet = 'Data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-facial",
   "metadata": {},
   "source": [
    "The imported data needs to be converted to `DatasetExperiment` format for use with `structToolbox`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sample meta data\n",
    "SM = data[ ,1:8]\n",
    "\n",
    "# add coloumn for sample type (QC, blank etc)\n",
    "blanks=c(1,2,33,34,65,66)\n",
    "QCs=c(3,4,11,18,25,32,35,36,43,50,57,64)\n",
    "SM$sample_type='Sample'\n",
    "SM$sample_type[blanks]='Blank'\n",
    "SM$sample_type[QCs]='QC'\n",
    "\n",
    "# put qc/blank labels in all factors for plotting later\n",
    "SM$biol.batch[SM$sample_type!='Sample']=SM$sample_type[SM$sample_type!='Sample']\n",
    "SM$time.point[SM$sample_type!='Sample']=SM$sample_type[SM$sample_type!='Sample']\n",
    "SM$condition[SM$sample_type!='Sample']=SM$sample_type[SM$sample_type!='Sample']\n",
    "\n",
    "# convert to factors\n",
    "SM$biol.batch=ordered(SM$biol.batch,c('9','10','11','12','QC','Blank'))\n",
    "SM$time.point=ordered(SM$time.point,c('0h','2h','6h','12h','18h','24h','QC','Blank'))\n",
    "SM$condition=factor(SM$condition)\n",
    "SM$sample_type=factor(SM$sample_type)\n",
    "\n",
    "# variable meta data\n",
    "VM = data.frame('annotation'=colnames(data)[9:ncol(data)])\n",
    "\n",
    "# raw data\n",
    "X = data[,9:ncol(data)]\n",
    "# convert 0 to NA\n",
    "X[X==0]=NA\n",
    "# force to numeric; any non-numerics will become NA\n",
    "X=data.frame(lapply(X,as.numeric),check.names = FALSE)\n",
    "\n",
    "# make sure row/col names match\n",
    "rownames(X)=data$label\n",
    "rownames(SM)=data$label\n",
    "rownames(VM)=colnames(X)\n",
    "\n",
    "\n",
    "# create DatasetExperiment object\n",
    "DE = DatasetExperiment(\n",
    "  data = X, \n",
    "  sample_meta = SM, \n",
    "  variable_meta = VM, \n",
    "  name = 'STATegra Metabolomics LCMS',\n",
    "  description = 'https://www.nature.com/articles/s41597-019-0202-7'\n",
    ")\n",
    "\n",
    "DE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "printable-equivalent",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "In the STATegra project the LCMS data was combined with GCMS data and multiblock\n",
    "analysis was conducted. Here only the LCMS will be explored, so the data will be processed\n",
    "differently in comparison to [Gomez-Cabrero et al](https://www.nature.com/articles/s41597-019-0202-7). Some basic processing steps will be applied in order to generate a valid PCA plot from the biological and QC samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-census",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare model sequence\n",
    "MS = filter_smeta(mode = 'include', levels='QC', factor_name = 'sample_type') +\n",
    "     knn_impute(neighbours=5) +\n",
    "     vec_norm() + \n",
    "     log_transform(base = 10) \n",
    "\n",
    "# apply model sequence\n",
    "MS = model_apply(MS, DE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-taxation",
   "metadata": {},
   "source": [
    "### Exploratory analysis\n",
    "First we will use PCA to look at the QC samples in order to make an assessment of the data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca model sequence\n",
    "M =  mean_centre() +\n",
    "     PCA(number_components = 3)\n",
    "\n",
    "# apply model\n",
    "M = model_apply(M,predicted(MS))\n",
    "\n",
    "\n",
    "# PCA scores plot\n",
    "C = pca_scores_plot(factor_name = 'sample_type',label_factor = 'order',points_to_label = 'all')\n",
    "# plot\n",
    "chart_plot(C,M[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-surprise",
   "metadata": {},
   "source": [
    "The QC labelled \"36\" is clearly very different to the other QCs. In STATegra this QC was removed, so we will exclude it here as well. This corresponds to QC H1. STATegra also excluded QC samples measured immediately after a blank, which we will also do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare mdoel sequence\n",
    "MS = filter_smeta(\n",
    "      mode = 'include', \n",
    "      levels='QC', \n",
    "      factor_name = 'sample_type') +\n",
    "  \n",
    "     filter_by_name(\n",
    "      mode = 'exclude', \n",
    "      dimension='sample',\n",
    "      names = c('1358BZU_0001QC_H1','1358BZU_0001QC_A1','1358BZU_0001QC_G1')) +\n",
    "  \n",
    "     knn_impute(\n",
    "      neighbours=5) +\n",
    "  \n",
    "     vec_norm() + \n",
    "  \n",
    "     log_transform(\n",
    "       base = 10) + \n",
    "  \n",
    "     mean_centre() +\n",
    "  \n",
    "     PCA(\n",
    "       number_components = 3)\n",
    "\n",
    "# apply model sequence\n",
    "MS = model_apply(MS, DE)\n",
    "\n",
    "# PCA scores plot\n",
    "C = pca_scores_plot(factor_name = 'sample_type',label_factor = 'order',points_to_label = 'all')\n",
    "# plot\n",
    "chart_plot(C,MS[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-float",
   "metadata": {},
   "source": [
    "Now we will plot the QC samples in context with the samples. There are several possible approaches, and we will apply the approach of applying PCA to the full dataset including the QCs. We will exclude the blanks as it is likely that they will dominate the plot if not removed. All samples from batch 12 were excluded from STATegra and we will replicate that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-bradley",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# prepare model sequence\n",
    "MS = filter_smeta(\n",
    "      mode = 'exclude', \n",
    "      levels='Blank', \n",
    "      factor_name = 'sample_type') +\n",
    "  \n",
    "     filter_smeta(\n",
    "      mode = 'exclude', \n",
    "      levels='12', \n",
    "      factor_name = 'biol.batch') +\n",
    "  \n",
    "     filter_by_name(\n",
    "      mode = 'exclude', \n",
    "      dimension='sample',\n",
    "      names = c('1358BZU_0001QC_H1',\n",
    "                '1358BZU_0001QC_A1',\n",
    "                '1358BZU_0001QC_G1')) +\n",
    "  \n",
    "     knn_impute(\n",
    "      neighbours=5) +\n",
    "  \n",
    "     vec_norm() + \n",
    "  \n",
    "     log_transform(\n",
    "       base = 10) + \n",
    "  \n",
    "     mean_centre() +\n",
    "  \n",
    "     PCA(\n",
    "       number_components = 3)\n",
    "\n",
    "# apply model sequence\n",
    "MS = model_apply(MS, DE)\n",
    "\n",
    "# PCA scores plots\n",
    "C = pca_scores_plot(factor_name = 'sample_type')\n",
    "# plot\n",
    "chart_plot(C,MS[8])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-france",
   "metadata": {},
   "source": [
    "The QCs appear to representative of the samples, but there are strong clusters in the data, including the QC samples which have no biological variation. There is likely to be a number of 'low quality' features that should be excluded, so we will do that now, and use more sophisticated normalisation (PQN) and scaling methods (glog)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-character",
   "metadata": {
    "fig.height": 10,
    "fig.width": 10
   },
   "outputs": [],
   "source": [
    "\n",
    "MS =  filter_smeta(\n",
    "       mode = 'exclude', \n",
    "       levels = '12', \n",
    "       factor_name = 'biol.batch') +\n",
    "  \n",
    "      filter_by_name(\n",
    "       mode = 'exclude', \n",
    "       dimension='sample',\n",
    "       names = c('1358BZU_0001QC_H1',\n",
    "                 '1358BZU_0001QC_A1',\n",
    "                 '1358BZU_0001QC_G1')) +\n",
    "\n",
    "      blank_filter(\n",
    "       fold_change = 20,\n",
    "       qc_label = 'QC',\n",
    "       factor_name = 'sample_type') +\n",
    "\n",
    "      filter_smeta(\n",
    "       mode='exclude',\n",
    "       levels='Blank',\n",
    "       factor_name='sample_type') +\n",
    "  \n",
    "      mv_feature_filter(\n",
    "       threshold = 80, \n",
    "       qc_label = 'QC', \n",
    "       factor_name = 'sample_type', \n",
    "       method = 'QC') +\n",
    "     \n",
    "      mv_feature_filter(\n",
    "        threshold = 50, \n",
    "        factor_name = 'sample_type', \n",
    "        method='across') +\n",
    "  \n",
    "     rsd_filter(\n",
    "       rsd_threshold=20, \n",
    "       qc_label='QC',\n",
    "       factor_name='sample_type') +\n",
    "  \n",
    "     mv_sample_filter(\n",
    "       mv_threshold = 50) +\n",
    "     \n",
    "     pqn_norm(\n",
    "       qc_label='QC',\n",
    "       factor_name='sample_type') +\n",
    "     \n",
    "     knn_impute(\n",
    "       neighbours=5, \n",
    "       by='samples') +\n",
    "     \n",
    "     glog_transform(\n",
    "       qc_label = 'QC',\n",
    "       factor_name = 'sample_type') +\n",
    "     \n",
    "     mean_centre() + \n",
    "     \n",
    "     PCA(\n",
    "       number_components = 10)\n",
    "\n",
    "# apply model sequence\n",
    "MS = model_apply(MS, DE)\n",
    "\n",
    "\n",
    "# PCA plots using different factors\n",
    "g=list()\n",
    "for (k in c('order','biol.batch','time.point','condition')) {\n",
    "  C = pca_scores_plot(factor_name = k,ellipse='none')\n",
    "  # plot\n",
    "  g[[k]]=chart_plot(C,MS[length(MS)])\n",
    "}\n",
    "\n",
    "plot_grid(plotlist = g,align='vh',axis='tblr',nrow=2,labels=c('A','B','C','D'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-proposal",
   "metadata": {},
   "source": [
    "We can see now that the QCs are tightly clustered. This indicates that the biological variance of the remaining high quality features is much greater than the technical variance represented by the QCs.\n",
    "\n",
    "There does not appear to be a trend by measurement order (A), which is an important indicator that instrument drift throughout the run is not a large source of variation in this dataset.\n",
    "\n",
    "There does not appear to be strong clustering related to biological batch (B).\n",
    "\n",
    "There does not appear to be a strong trend with time (C) but this is likely to be a more subtle variation and might be masked by other sources of variance at this stage.\n",
    "\n",
    "There is some clustering related to condition (D) but with overlap.\n",
    "\n",
    "To further explore any trends with time, we will split the data by the condition factor and only explore the Ikaros group. Removing the condition factor variation will potentially make it easier to spot any more subtle trends. We will extract the glog transformed matrix from the previous model sequence and continue from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-application",
   "metadata": {
    "fig.height": 11,
    "fig.width": 5,
    "message": false,
    "warning": false
   },
   "outputs": [],
   "source": [
    "# get the glog scaled data\n",
    "GL = predicted(MS[11])\n",
    "\n",
    "# extract the Ikaros group and apply PCA\n",
    "IK = filter_smeta(\n",
    "      mode='include',\n",
    "      factor_name='condition',\n",
    "      levels='Ikaros') +\n",
    "     mean_centre() + \n",
    "     PCA(number_components = 5)\n",
    "\n",
    "# apply the model sequence to glog transformed data\n",
    "IK = model_apply(IK,GL)\n",
    "\n",
    "# plot the PCA scores\n",
    "C = pca_scores_plot(factor_name='time.point',ellipse = 'sample')\n",
    "g1=chart_plot(C,IK[3])\n",
    "g2=g1 + scale_color_viridis_d() # add continuous scale colouring\n",
    "\n",
    "plot_grid(g1,g2,nrow=2,align='vh',axis = 'tblr',labels=c('A','B'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-cigarette",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Colouring by groups (A) makes the time point trend difficult to see, but by adding a `ggplot` continuous colour scale \"viridis\" (B) the trend with time along PC1 becomes much clearer.\n",
    "\n",
    "# Session Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionInfo()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,message,fig.width,results,fig.wide,eval,include,warning,fig.height,-all",
   "main_language": "R",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
    "name": "ir",
    "display_name": "R",
    "language": "R"
  },
  "language_info": {
    "name": "R",
    "codemirror_mode": "r",
    "pygments_lexer": "r",
    "mimetype": "text/x-r-source",
    "file_extension": ".r",
    "version": "4.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
